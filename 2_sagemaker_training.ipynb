{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e20d5ea",
   "metadata": {},
   "source": [
    "![](display_images/remars_logo.png)\n",
    "\n",
    "# Train a 3D object detector\n",
    "\n",
    "Welcome to the training notebook! In the previous notebook you learned about the [A2D2 Dataset](https://www.a2d2.audi/a2d2/en.html), how to visualize it, and how to launch an Amazon SageMaker Ground Truth Labeling Job. \n",
    "\n",
    "In this notebook we will walk through how to train a 3D object detection model using Amazon SageMaker. We will:\n",
    "- Build a custom container\n",
    "- Setup FSx for Lustre as a data source\n",
    "- Setup SageMaker Experiments\n",
    "- Launch a distributed training job on Amazon SageMaker\n",
    "- Review training job profiling metrics \n",
    "\n",
    "Training a 3D object detection model requires a specialized toolset. Point cloud data cannot simply use the same kinds of operations 2D vision models use out of the box. Point cloud data tends to be rather sparse and spread out. The typical way 3D object detection models handle point cloud data is by either using specialized [sparse convolutions](https://arxiv.org/pdf/1711.10275.pdf) or by [voxelizing (generating uniform 3D boxes)](https://arxiv.org/pdf/1711.06396.pdf) the input. [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) has implementations of a variety of different 3D object detection and segmentation models, making model training much easier! We are going to use a model called [3DSSD](https://arxiv.org/pdf/2002.10187.pdf). Some of you who are familiar with the 2D version of SSD will not notice a lot of similarities. It is indeed a single shot detector, but the feature generation it uses is much different, read the paper to learn more about it!\n",
    "\n",
    "We will start by installing [SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) and cloning MM3D into our local filesystem.\n",
    "\n",
    "Note: **Please use conda_pytorch_p38 as the kernel for this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e536557",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker-experiments\n",
    "!cd container_training && git clone https://github.com/open-mmlab/mmdetection3d.git\n",
    "# !pip install botocore==1.24.42\n",
    "!pip install botocore==1.29.114\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af1bfa",
   "metadata": {},
   "source": [
    "To install MM3D in your local environment, run the following commands. We aren't going to install MM3D in our kernel because the process takes ~20 minutes and can be complicated by CUDA dependencies. Instead we will build MM3D in our docker image where we can explicitly control the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8f3c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## If installing MM3D in your local environment, make sure the CUDA version you use matches the CUDA version PyTorch and MMCV are built with.\n",
    "\n",
    "# %%time\n",
    "# !pip install -U torch==1.8.1 torchvision==0.9.1\n",
    "# !export MKL_SERVICE_FORCE_INTEL=1 && pip install mmcv-full==1.3.13 -f https://download.openmmlab.com/mmcv/dist/cu110/torch1.8.1/index.html\n",
    "# !pip install mmdet==2.17.0\n",
    "# !pip install mmsegmentation==0.18.0\n",
    "# !cd container_training && git clone https://github.com/iprivit/mmdetection3d.git\n",
    "# !export MKL_SERVICE_FORCE_INTEL=1 && cd container_training/mmdetection3d && pip install -v -e .\n",
    "\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822b420b",
   "metadata": {},
   "source": [
    "### Initialize clients and import libraries\n",
    "\n",
    "Let's import a few libraries and initialize some [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e98d84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import tarfile\n",
    "import boto3\n",
    "import sagemaker\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import patches\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "# import torch\n",
    "# import torchvision\n",
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile, DetailedProfilingConfig, DataloaderProfilingConfig, PythonProfilingConfig, Rule, ProfilerRule, rule_configs\n",
    "\n",
    "def timestamp_to_utc(timestamp):\n",
    "    utc_dt = datetime.utcfromtimestamp(timestamp)\n",
    "    return utc_dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# set device for PyTorch to use, if on a GPU instance use cuda\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialize clients to make boto3 calls \n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "b3sess = boto3.Session()\n",
    "fsx_client = boto3.client('fsx')\n",
    "cfn_client = boto3.client('cloudformation')\n",
    "sm = b3sess.client('sagemaker')\n",
    "region = b3sess.region_name\n",
    "\n",
    "# set the S3 bucket you'll use\n",
    "bucket = sagemaker_session.default_bucket() \n",
    "prefix_output = 'training_res'\n",
    "\n",
    "# # Use cloudformation to describe the stack we've built to grab resource names \n",
    "# stack_res = cfn_client.describe_stack_resources(\n",
    "#     StackName='threedee',\n",
    "# )\n",
    "# resource_dict = {}\n",
    "# for stack in stack_res['StackResources']:\n",
    "#     resource_dict[stack['ResourceType']] = stack['PhysicalResourceId']\n",
    "    \n",
    "# # grab subnets and security groups so we can run our training in our VPC\n",
    "# subnets = [resource_dict['AWS::EC2::Subnet']]\n",
    "# security_group_ids = [resource_dict['AWS::EC2::SecurityGroup']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554c4d6",
   "metadata": {},
   "source": [
    "### View Dockerfile\n",
    "\n",
    "Since MMDetection3D has rather complex dependencies, the easiest way to install it in our environment is by using [Docker](https://www.docker.com/resources/what-container). Docker allows us to create self contained images with all of the dependencies necessary to run MMDetection3D. Let's take a look at the Dockerfile we are going to use to build our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00eca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view our dockerfile\n",
    "!pygmentize -l docker container_training/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8a212",
   "metadata": {},
   "source": [
    "## Build Our Docker Container\n",
    "\n",
    "Now that we have taken a look at our Dockerfile, let's build our container and push it to [Amazon Elastic Container Registry (ECR)](https://aws.amazon.com/ecr/). We will build our container, create a repository for it in ECR, and push our image to ECR with one simple command. This will allow us to later use that container when we run our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49d281",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMAGE_NAME = 'mmdet3d-sagemaker-pt181' \n",
    "# account = '579019700964'  # boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# # # if in MLR401 your container will already be built for you, you can ue the following commands to pull them down!\n",
    "\n",
    "# # ! docker pull public.ecr.aws/k2j9l5n0/mmdet3d\n",
    "# # ! docker tag public.ecr.aws/k2j9l5n0/mmdet3d {account}.dkr.ecr.us-east-1.amazonaws.com/{IMAGE_NAME}\n",
    "# # ! aws ecr get-login --no-include-email | bash\n",
    "# # ! aws ecr create-repository --region {region} --repository-name {IMAGE_NAME}\n",
    "# # ! docker push {account}.dkr.ecr.us-east-1.amazonaws.com/{IMAGE_NAME}\n",
    "\n",
    "# # # if running on your own uncomment out the below lines:\n",
    "# !aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "# !bash ./build_and_push.sh {region} {IMAGE_NAME} latest container_training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06494e-e7bf-4f71-a4ca-a95c4739729e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_NAME = 'mmdet3d-sagemaker-pt1131' \n",
    "account = '579019700964'\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "!bash ./build_and_push.sh {region} {IMAGE_NAME} latest container_training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feea1ad",
   "metadata": {},
   "source": [
    "Let's unpack our ground truth labels, DO NOT SKIP THIS STEP! Otherwise your model will not have any labels to train with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bfe0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unpack the ground truth labels\n",
    "\n",
    "!aws s3 cp s3://aws-tc-largeobjects/DEV-AWS-MO-Nvidia/a2d2_gt_database.tar.gz ../fsx/a2d2/a2d2_gt_database.tar.gz\n",
    "!tar -xzf ../fsx/a2d2/a2d2_gt_database.tar.gz -C ../fsx/a2d2/camera_lidar_semantic_bboxes/\n",
    "!cp a2d2/a2d2*.pkl ../fsx/a2d2/camera_lidar_semantic_bboxes/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1449a58",
   "metadata": {},
   "source": [
    "### Pre-process point clouds\n",
    "\n",
    "One pre-processing step we need to take care of is converting our LiDAR point clouds into bin files our [DataLoader](https://pytorch.org/docs/stable/data.html) is expecting. A2D2 stores point cloud data in [NPZ](https://numpy.org/doc/stable/reference/generated/numpy.savez.html) files, which are compressed Numpy files. We will use the convert_lidar function we define below and parallelize it's execution by using the [process_map](https://tqdm.github.io/docs/contrib.concurrent/) utility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545feb6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_lidar(lidar_path, debug = False):\n",
    "\n",
    "    \"\"\"\n",
    "    azimuth     -\n",
    "    row         - 2d coordinate of LiDAR point in image space, y (1208)\n",
    "    col         - 2d coordinate of LiDAR point in image space, x (1920)\n",
    "    lidar_id    - lidar sensor id (5)\n",
    "    depth       -\n",
    "    reflectance - reflectance measurement\n",
    "    points      - 3D point measurement\n",
    "    timestamp   -\n",
    "    distance    -\n",
    "    \"\"\"\n",
    "\n",
    "    lidar = np.load(lidar_path)\n",
    "    xyz   = lidar['points'     ].astype(dtype = np.float32)\n",
    "    i     = lidar['reflectance'].astype(dtype = np.float32).reshape(-1, 1)\n",
    "    xyzi  = np.concatenate((xyz, i), axis = 1) # [x y z] + [i]\n",
    "\n",
    "    if  debug:\n",
    "\n",
    "        pprint(xyz)\n",
    "        pprint(i)\n",
    "        pprint(xyzi)\n",
    "\n",
    "        pprint(np.asarray(np.unique(lidar['lidar_id'], return_counts = True), dtype = int).T)\n",
    "    \n",
    "    path  = lidar_path.replace('npz', 'bin')\n",
    "    xyzi.ravel().tofile(path) # flatten\n",
    "#     print(path)\n",
    "\n",
    "roots = glob('../fsx/a2d2/camera_lidar_semantic_bboxes/2*')\n",
    "for root in tqdm(roots):\n",
    "    paths = glob(f'{root}/lidar/cam_front_center/*npz')\n",
    "    process_map(convert_lidar, paths, max_workers = multiprocessing.cpu_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dbf005",
   "metadata": {},
   "source": [
    "## Create metric definitions\n",
    "\n",
    "Since we aren't training on the same instance our notebook is hosted on we need a way to capture our performance metrics. SageMaker allows users to collect metrics from the output logs of their training jobs. In our case we are going to capture the 4 loss outputs from our Faster RCNN model as well as the total loss, the learning rate, and the number of training iterations. The following definition specifies the name of the metric collected and the appropriate regex used to collect the metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46de635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define metrics\n",
    "\n",
    "metric_definitions=[{\n",
    "        \"Name\": \"loss\",\n",
    "        \"Regex\": \".*loss:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"corner_loss\",\n",
    "        \"Regex\": \".*corner_loss:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"vote_loss\",\n",
    "        \"Regex\": \".*vote_loss:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"dir_class_loss\",\n",
    "        \"Regex\": \".*dir_class_loss:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"lr\",  \n",
    "        \"Regex\": \".*lr:\\s([0-9\\\\.]+)\\s*\"\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"grad_norm\",  \n",
    "        \"Regex\": \".*grad_norm:\\s([0-9\\\\.]+)\\s*\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dc8f7",
   "metadata": {},
   "source": [
    "## SageMaker Experiments \n",
    "\n",
    "Now that we have specified our training metrics above, we are going to need a way to organize and compare our training runs. [Amazon SageMaker experiments](https://aws.amazon.com/blogs/aws/amazon-sagemaker-experiments-organize-track-and-compare-your-machine-learning-trainings/) lets you organize, track, compare and evaluate machine learning experiments and model versions. We can add experiments tracking to our training jobs using a couple simple hooks. There is a small amount of setup required before we can hook it into our estimators. We first are going to create our experiment, and within our experiment create a trial for our new training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85399945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create d2 experiment\n",
    "\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "from smexperiments.search_expression import Filter, Operator, SearchExpression\n",
    "\n",
    "mm3d_experiment = Experiment.create(\n",
    "    experiment_name=f\"mm3d-a2d2-demo-{int(time.time())}\", \n",
    "    description=\"MMDetection3D training on the A2D2 dataset\", \n",
    "    sagemaker_boto_client=sm)\n",
    "print(mm3d_experiment,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2745e9d",
   "metadata": {},
   "source": [
    "### Setup FSx for Lustre\n",
    "\n",
    "We created a [FSx for Lustre filesystem](https://aws.amazon.com/fsx/lustre/) when we launched our initial [Cloudformation](https://aws.amazon.com/cloudformation/) stack. FSx for Lustre is a high performance file system that provides fast, scalable storage. It's ideal for tasks that require high data throughput like distributed training. \n",
    "\n",
    "In order to mount it to our SageMaker Training instance, we will need to create a FileSystemInput object. This will tell SageMaker how and where to mount the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d347a0e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure FSx Input for your SageMaker Training job\n",
    "# from sagemaker.inputs import FileSystemInput\n",
    "# username = 'AWS'\n",
    "\n",
    "# file_system_id= resource_dict['AWS::FSx::FileSystem']  # FSx file system ID with your training dataset. Example: 'fs-0bYYYYYY'\n",
    "# file_system_directory_path= f\"/{fsx_client.describe_file_systems()['FileSystems'][0]['LustreConfiguration']['MountName']}/a2d2\"  # NOTE: '/fsx/' will be the root mount path. Example: '/fsx/mask_rcnn/PyTorch'\n",
    "# file_system_access_mode='ro'\n",
    "# file_system_type='FSxLustre'\n",
    "# train_fs = FileSystemInput(file_system_id=file_system_id,\n",
    "#                                     file_system_type=file_system_type,\n",
    "#                                     directory_path=file_system_directory_path,\n",
    "#                                     file_system_access_mode=file_system_access_mode)\n",
    "# if using the above FSx Input then use the following data channel config\n",
    "# data_channels = {'train': train_fs}\n",
    "# if using local mode gpu training use the following data channel instead:\n",
    "data_channels =  {'train': 'file://../fsx/a2d2'} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b5e3a",
   "metadata": {},
   "source": [
    "# SageMaker Training\n",
    "\n",
    "![](display_images/sagemaker_how_it_works.png)\n",
    "\n",
    "[Image source](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca369bb3",
   "metadata": {},
   "source": [
    "## Define the Estimator\n",
    "\n",
    "In SageMaker training jobs are created by initializing an estimator class where we define our training container, our entrypoint, our hyperparameters and instance types in addition to a few other variables and then launching our training job on the instance or instances we specify.\n",
    "\n",
    "We first define a set of hyperparameters that we will pass to our estimator. When we launch our training job, these hyperparameters in addition to any source directory we define, will be packaged up and uploaded to our training instance running our Docker image. \n",
    "\n",
    "We then define our profiler configuration. SageMaker Debugger allows data scientists the ability to debug, monitor, and profile training jobs in real time! In this notebook we will focus specifically on the profiling. SageMaker Debugger's profiling feature allows us to collect both system and framework level information about our training job. This gives us information ranging from CPU/GPU utilization to detailed descriptions of the most time consuming operations in our training job! When we setup our profiling configuration, we tell our estimator how often to record both system and framework level information on our training job.\n",
    "\n",
    "For our specific training job, MM3D has a wide variety of model architectures with pretrained weights that we can use as a starting point. In our hyperparameters we can define the configuration file that tells the MM3D framework what model architecture we want to use. In this case we are using a 3DSSD model, you can experiment with different models, but make sure to look at how they ingest data first.  \n",
    "\n",
    "Our training script is setup for distributed training so let's launch our job on one of AWS's multi-gpu instances!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec087aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = '3dssd/3dssd_4x4_a2d2-3d-car.py'\n",
    "launcher = 'none' # if using distributed training set to pytorch, otherwise if using single GPU, set to none\n",
    "\n",
    "with open(f\"container_training/mmdetection3d/configs/{config.split('/')[0]}/metafile.yml\", 'r') as f:\n",
    "    cfg_meta = yaml.safe_load(f)\n",
    "    \n",
    "model_path = cfg_meta['Models'][0]['Weights']\n",
    "!wget {model_path} -O container_training/{model_path.split('/')[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14432d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run training job\n",
    "\n",
    "# create experiment trial\n",
    "trial_name = f\"mm3d-demo-training-job-{int(time.time())}\"\n",
    "mm3d_trial = Trial.create(\n",
    "    trial_name=trial_name, \n",
    "    experiment_name=mm3d_experiment.experiment_name,\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "print(mm3d_trial)\n",
    "\n",
    "account = boto3.client('sts').get_caller_identity()['Account']\n",
    "image_uri = f'{account}.dkr.ecr.us-east-1.amazonaws.com/{IMAGE_NAME}'\n",
    "\n",
    "# pick our instance type\n",
    "instance_type = 'local_gpu' # set to use local mode, but if running in your own account, try running one of the below larger instances:\n",
    "if instance_type in ['ml.p3.8xlarge', 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.g4dn.12xlarge']:\n",
    "    distributed = 1\n",
    "else:\n",
    "    distributed = 0\n",
    "\n",
    "# set our hyperparameters\n",
    "hyperparameters = {\n",
    "    'config': f'/mmdetection3d/configs/{config}', \n",
    "    \"work-dir\":'/opt/ml/model/',\n",
    "    'launcher':launcher,\n",
    "    # 'load-path':f\"/opt/ml/code/{model_path.split('/')[-1]}\",\n",
    "    # 'resume':f\"/opt/ml/code/{model_path.split('/')[-1]}\",\n",
    "    \"distributed\":distributed,\n",
    "    \"epochs\":1,\n",
    "    \"batch-size\":6,\n",
    "    \"instance-count\":1,\n",
    "    # \"auto-scale-lr\": '',\n",
    "}\n",
    "\n",
    "# setup our SageMaker Debugger Profiler configuration to monitor our resource utilization\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=1000,\n",
    ")\n",
    "\n",
    "# setup our estimator\n",
    "estimator = PyTorch(\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type= instance_type,\n",
    "                      entry_point='train.py',\n",
    "                      source_dir='container_training',\n",
    "                      image_uri=image_uri,\n",
    "                      volume_size=225,\n",
    "                      output_path=f\"s3://{bucket}/{prefix_output}\",\n",
    "                      base_job_name=f\"{config.split('/')[0]}-{launcher}-{instance_type.replace('.','-')}\", \n",
    "                      profiler_config=profiler_config,\n",
    "                      enable_cloudwatch_metrics=True,\n",
    "                      hyperparameters=hyperparameters,\n",
    "                      metric_definitions=metric_definitions,\n",
    "                      # subnets=subnets,\n",
    "                      # security_group_ids=security_group_ids,\n",
    "#                       distribution={  # if running distributed training, uncomment this argument\n",
    "#                         \"mpi\":{\"enabled\":True}\n",
    "#                       },\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c9191",
   "metadata": {},
   "source": [
    "Now that we defined our estimator, we can launch our training job. We supply a few arguments, including \n",
    "- `inputs` this argument informs SageMaker where to find our training data.\n",
    "- `experiment_config` this argument is where we specify an experiment configuration for SageMaker Experiments.\n",
    "- `wait` this argument defines whether we want to hold the attention of the notebook cell. In this case we are setting it to `True` so that we can view the log output of our training job in our notebook.\n",
    "\n",
    "If you receive this error: `CapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.`, this is because we are running our training inside of a specific [availability zone or AZ](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-availability-zones) in our [virtual private cloud or VPC](https://aws.amazon.com/vpc/). Capacity is constantly being replenished, so wait a minute or two and retry launching your training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c5406",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit(inputs=data_channels, \n",
    "              wait=True,\n",
    "              experiment_config={\n",
    "            \"ExperimentName\": mm3d_experiment.experiment_name,\n",
    "            \"TrialName\": mm3d_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": f\"Training-{instance_type.replace('.','-')}\"},\n",
    "                  )\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print('Training job name:', training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3edab0d",
   "metadata": {},
   "source": [
    "To run training locally on your instance inside of the docker image we pulled from ECR, run the output of the following print commands in a terminal, the gif below will demonstrate how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5801d3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # to run the same training job locally on your SageMaker instance, run the following commands in a terminal:\n",
    "\n",
    "# print(f'docker run -it --gpus all -v /home/ec2-user/SageMaker/fsx/a2d2:/opt/ml/input/data/train {account}.dkr.ecr.us-east-1.amazonaws.com/mmdet3d-sagemaker-pt181 bash')\n",
    "# print(f'cd /opt/ml/code && python train.py --config /mmdetection3d/configs/3dssd/3dssd_4x4_a2d2-3d-car.py --batch-size 8 --epochs 1') # # takes about 45 minutes\n",
    "# print('cp /opt/ml/code/work_dirs/3dssd_4x4_a2d2-3d-car/latest.pth /opt/ml/input/data/train/model.pth') \n",
    "# print('exit')\n",
    "# print('cd /home/ec2-user/SageMaker/end-2-end-3d-ml && tar -cvf model.tar.gz ../fsx/a2d2/model.pth') # model will be deposited in end-2-end-3d-ml folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5843c0d-5559-4370-9795-6dbdf21784f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to run the same training job locally on your SageMaker instance, run the following commands in a terminal:\n",
    "\n",
    "print(f'docker run -it --gpus all -v /home/ec2-user/SageMaker/fsx/a2d2:/opt/ml/input/data/train {account}.dkr.ecr.us-east-1.amazonaws.com/mmdet3d-sagemaker-pt1131 bash')\n",
    "print(f'cd /opt/ml/code && python train.py --config /mmdetection3d/configs/3dssd/3dssd_4x4_a2d2-3d-car.py') # # takes about 45 minutes\n",
    "print('cp /opt/ml/code/work_dirs/3dssd_4x4_a2d2-3d-car/latest.pth /opt/ml/input/data/train/model.pth') \n",
    "print('exit')\n",
    "print('cd /home/ec2-user/SageMaker/end-2-end-3d-ml && tar -cvf model.tar.gz ../fsx/a2d2/model.pth') # model will be deposited in end-2-end-3d-ml folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83bca6",
   "metadata": {},
   "source": [
    "![](display_images/local_train.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc71caf",
   "metadata": {},
   "source": [
    "All of the information that Debugger gathers is stored in s3. The below call via the AWS CLI will check if profiler information has been saved to our training job's folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb0a66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! aws s3 ls s3://{bucket}/{prefix_output}/{training_job_name}/profiler-output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375d44c",
   "metadata": {},
   "source": [
    "### Download our model object\n",
    "\n",
    "We will use this later when we deploy our model as an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2b84ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://{bucket}/{prefix_output}/{training_job_name}/output/model.tar.gz ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c05b3d8",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red;font-size:22.0pt\">  IF YOU RAN THE TRAINING JOB USING LOCAL MODE THE FOLLOWING BLOCKS WILL NOT WORK.</span>\n",
    "\n",
    "### Find system metrics\n",
    "\n",
    "Once our outputs have been processed, we can import our system and framework data into our notebook and visualize them! The following block will check for the availability of our profiling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c75305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.system_metrics_reader import S3SystemMetricsReader\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "output_path = f's3://{bucket}/{prefix_output}/{training_job_name}/profiler-output'\n",
    "print(f'output path: {output_path}')\n",
    "print(f'Training job name: {training_job_name}')\n",
    "\n",
    "system_metrics_reader = S3SystemMetricsReader(output_path)\n",
    "\n",
    "training_job_status = ''\n",
    "training_job_secondary_status = ''\n",
    "while system_metrics_reader.get_timestamp_of_latest_available_file() == 0:\n",
    "    system_metrics_reader.refresh_event_file_list()\n",
    "    client = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=training_job_name\n",
    "    )\n",
    "    if 'TrainingJobStatus' in client:\n",
    "        training_job_status = f\"TrainingJobStatus: {client['TrainingJobStatus']}\"\n",
    "    if 'SecondaryStatus' in client:\n",
    "        training_job_secondary_status = f\"TrainingJobSecondaryStatus: {client['SecondaryStatus']}\"\n",
    "        \n",
    "    print(f\"Profiler data from system not available yet. {training_job_status}. {training_job_secondary_status}.\")\n",
    "    time.sleep(20)\n",
    "\n",
    "print('\\n\\nProfiler data from system is available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e12765",
   "metadata": {},
   "source": [
    "## Visualize Data in Notebook\n",
    "\n",
    "Now that we have verified our profiler data is available, let's plot some system metrics in our notebook. One easy thing to check for is if you are fully utilizing your GPU memory. If it seems low, we might be able to increase our batch size! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb12a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create system plots\n",
    "\n",
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "from smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts\n",
    "tj = TrainingJob(training_job_name, region)\n",
    "tj.wait_for_sys_profiling_data_to_be_available()\n",
    "\n",
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "\n",
    "view_timeline_charts  = TimelineCharts(system_metrics_reader, \n",
    "                                       framework_metrics_reader=None,\n",
    "                                       select_dimensions=[\"CPU\", \"GPU\"], \n",
    "                                       select_events=[\"total\"] # if you want to look specifically at gpu0 and gpu1, replace total with a list\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383116c1",
   "metadata": {},
   "source": [
    "### Create a heatmap\n",
    "\n",
    "This heatmap shows similar system utilization metrics but different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcba224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.profiler.analysis.notebook_utils.heatmap import Heatmap\n",
    "\n",
    "view_heatmap = Heatmap(\n",
    "    system_metrics_reader,\n",
    "#     framework_metrics_reader, # can add back in\n",
    "    select_dimensions=[\"CPU\", \"GPU\", \"I/O\"], # optional\n",
    "    select_events=[\"total\"],                 # optional\n",
    "    plot_height=450\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96033c5",
   "metadata": {},
   "source": [
    "## Access system level metrics\n",
    "\n",
    "We can look at our system level metrics in depth by using our system metric reader and pulling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184116e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get system metrics\n",
    "\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "last_timestamp = system_metrics_reader.get_timestamp_of_latest_available_file()\n",
    "events = system_metrics_reader.get_events(0, last_timestamp) \n",
    "\n",
    "print(\"Found\", len(events), \"recorded system metric events. Latest recorded event:\",  \n",
    "      timestamp_to_utc(last_timestamp/1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97d9d3",
   "metadata": {},
   "source": [
    "### Create system level metric dataframe\n",
    "\n",
    "We can create a dataframe with all of our system metrics for further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfb8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = []\n",
    "names = []\n",
    "node_ids = []\n",
    "timestamps = []\n",
    "types = []\n",
    "values = []\n",
    "for event in events:\n",
    "    dimensions.append(event.dimension)\n",
    "    names.append(event.name)\n",
    "    node_ids.append(event.node_id)\n",
    "    timestamps.append(event.timestamp)\n",
    "    types.append(event.type)\n",
    "    values.append(event.value)\n",
    "    \n",
    "system_df = pd.DataFrame.from_dict({\n",
    "    \"dimension\":dimensions,\n",
    "    \"name\":names,\n",
    "    \"node_id\":node_ids,\n",
    "    \"timestamp\":timestamps,\n",
    "    \"type\":types,\n",
    "    \"value\":values\n",
    "})\n",
    "\n",
    "system_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada6365",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_df.groupby(by='dimension').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50fd6e",
   "metadata": {},
   "source": [
    "## View rule output\n",
    "\n",
    "Profiler also generates a html and iPython notebook that goes over the different rules your job triggered. These rules give you hints about where your job could improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_output_path = f\"s3://{bucket}/{prefix_output}/{training_job_name}/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")\n",
    "\n",
    "!aws s3 cp --recursive {rule_output_path} ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b7b12",
   "metadata": {},
   "source": [
    "## Get experiment results\n",
    "\n",
    "Once our training job is complete, we want to evaluate the results of our training run. An easy way to do so is by using SageMaker Experiments. In addition to the UI in SageMaker Studio, SageMaker Experiments allows users to import their results as a dataframe so they can easily evaluate their training runs. The code below will associate our trial with our experiment we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3975575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timezone\n",
    "from smexperiments.search_expression import Filter, Operator, SearchExpression\n",
    "\n",
    "# get the trial components derived from the training jobs\n",
    "\n",
    "creation_time = estimator.latest_training_job.describe()['CreationTime'] #most_recently_created_tuning_job[\"CreationTime\"]\n",
    "creation_time = creation_time.astimezone(timezone.utc)\n",
    "creation_time = creation_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "created_after_filter = Filter(\n",
    "    name=\"CreationTime\",\n",
    "    operator=Operator.GREATER_THAN_OR_EQUAL,\n",
    "    value=str(creation_time),\n",
    ")\n",
    "\n",
    "# the training job names contain the tuning job name (and the training job name is in the source arn)\n",
    "source_arn_filter = Filter(\n",
    "    name=\"TrialComponentName\", operator=Operator.CONTAINS, value=training_job_name\n",
    ")\n",
    "source_type_filter = Filter(\n",
    "    name=\"Source.SourceType\", operator=Operator.EQUALS, value=\"SageMakerTrainingJob\"\n",
    ")\n",
    "\n",
    "search_expression = SearchExpression(\n",
    "    filters=[created_after_filter, source_arn_filter, source_type_filter]\n",
    ")\n",
    "\n",
    "# search iterates over every page of results by default\n",
    "trial_component_search_results = list(\n",
    "    TrialComponent.search(search_expression=search_expression, sagemaker_boto_client=sm)\n",
    ")\n",
    "print(f\"Found {len(trial_component_search_results)} trial components.\")\n",
    "trial_component_search_results\n",
    "\n",
    "# associate the trial components with the trial\n",
    "for tc in trial_component_search_results:\n",
    "    print(f\"Associating trial component {tc.trial_component_name} with trial {mm3d_trial.trial_name}.\")\n",
    "    mm3d_trial.add_trial_component(tc.trial_component_name)\n",
    "    # sleep to avoid throttling\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c91ff2",
   "metadata": {},
   "source": [
    "### View experiments DataFrame\n",
    "\n",
    "Once we have associated our experiment trials, we can import them as a DataFrame. We can incorporate search expressions to narrow down training runs with specific attributes and sort our trials by specified metrics. The experiments will track all of your set hyperparameters, making it easier to evaluate the effects of changing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a881be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sagemaker_session,  \n",
    "    experiment_name=mm3d_experiment.experiment_name,\n",
    "#     search_expression=search_expression,\n",
    "    sort_by=\"metrics.test:accuracy.max\",\n",
    "    sort_order=\"Descending\",\n",
    "#     metric_names=['test:accuracy'],\n",
    ")\n",
    "\n",
    "trial_df = trial_component_analytics.dataframe()\n",
    "trial_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb761d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we setup SageMaker Training with the MMDetection3D repository and FSx for Lustre as a data source. We trained our model on a multi-GPU instance and downloaded our model object. In the next notebook we will deploy the model we trained as an asynchronous SageMaker endpoint!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
