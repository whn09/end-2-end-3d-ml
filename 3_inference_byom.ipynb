{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac1946a",
   "metadata": {},
   "source": [
    "# SageMaker Inference with mmdetection3D\n",
    "\n",
    "This notebook will use the model you trained earlier in the workshop to perform inference using [mmdetection3D](https://github.com/open-mmlab/mmdetection3d).\n",
    "\n",
    "Point cloud data can take up a lot of space. Because pointcloud payloads may be quite large, we will use SageMaker's [Asynchronous Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html) capability. \n",
    "\n",
    "The notebook will follow these basic steps:\n",
    "\n",
    "* Define a SageMaker model using the results from our training process\n",
    "* Create an async inference endpoint\n",
    "* Test the endpoint\n",
    "\n",
    "## Async Inference\n",
    "\n",
    "SageMaker real-time inference endpoints typically act in a synchronous mode, just like any RESTful API.  That poses a challenge for cases where the input or output are too large for a normal REST payload.  SageMaker async inference endpoints solve that problem.\n",
    "\n",
    "![Async Inference](display_images/async-inf.png)\n",
    "\n",
    "_Image copied from a [SageMaker example notebook](https://github.com/aws/amazon-sagemaker-examples/blob/master/async-inference/Async-Inference-Walkthrough.ipynb)_\n",
    "\n",
    "When a request comes in to an async inference endpoint, the request goes into an internal queue, and references input data stored in an S3 bucket.  When the inference code runs, it gets the input data from S3 and gets the inference.  SageMaker stores the result in another S3 object, which we can download later.\n",
    "\n",
    "See this [example notebook](https://github.com/aws/amazon-sagemaker-examples/blob/master/async-inference/Async-Inference-Walkthrough.ipynb) for a more detailed walkthrough of async inference endpoints.\n",
    "\n",
    "### Inference I/O format\n",
    "\n",
    "Since we are providing our own inference code, we can determine the input and output formats.  The input to the inference endpoint is a point cloud from A2D2.  The response is a [pickled](https://docs.python.org/3/library/pickle.html) version of the mmdetect response object.\n",
    "\n",
    "The model artifact has to contain a configuration file and a model checkpoint.  The configuration file \n",
    "can point to other items in the `mmdetection3d` folder, but it has to contain absolute paths.  In the image,\n",
    "`mmdetection3d` is installed directly under the root.\n",
    "\n",
    "We have the mmdetect configuration file and model checkpoint prepared from the previous notebook.\n",
    "\n",
    "### Inference code\n",
    "\n",
    "Our inference container runs a [Flask](https://flask.palletsprojects.com/en/2.0.x/) server to respond to inference requests.  You can review the code in the `container_inference` directory.  The files `serve`, `nginx.conf`, and `wsgi.py` are boilerplate for the Flask server.  \n",
    "\n",
    "The interesting code is in `predictor.py`.\n",
    "\n",
    "Note: **Please use conda_pytorch_p38 as the kernel for this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3408b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize -l python container_inference/mm3d/predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac253a",
   "metadata": {},
   "source": [
    "You'll notice that the `predictor.py` code handles two Flask methods.  The first, `ping`, is just a health check that lets SageMaker know that the endpoint is ready to serve.  In that method, we see if we can load the model successfully.\n",
    "\n",
    "The second method, `transformation`, actually returns an inference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c0b18f",
   "metadata": {},
   "source": [
    "### Install pytransform3d\n",
    "\n",
    "We will install pytransform3d that we will use later to visualize our inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127b1c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pytransform3d -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794de6a1",
   "metadata": {},
   "source": [
    "## Upload model artifact\n",
    "\n",
    "When you train a model in SageMaker it packages the results in a compressed `model.tar.gz` file. This file contains the trained model weights and any model configuration files. SageMaker inference requires your model to be in this format, but since we trained our model in SageMaker it's already in the correct format. Let's upload this file to S3 so our endpoint can retrieve it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3710ac7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the default notebook execution role\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9390c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# get sagemaker runtime\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Upload model artifact to S3\n",
    "file_key = 'model.tar.gz'\n",
    "sm_session = sagemaker.session.Session()\n",
    "bucket = sm_session.default_bucket()\n",
    "prefix = \"mm3d\"\n",
    "b3sess = boto3.Session()\n",
    "region = b3sess.region_name\n",
    "try:\n",
    "    model_artifact = S3Uploader.upload(file_key, f's3://{bucket}/{prefix}/model')\n",
    "except:\n",
    "    !aws s3 cp s3://nvidia-aws-coursera/model.tar.gz .\n",
    "    model_artifact = S3Uploader.upload(file_key, f's3://{bucket}/{prefix}/model')\n",
    "    \n",
    "print(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade431a",
   "metadata": {},
   "source": [
    "### Deployment container\n",
    "\n",
    "If running this in an AWS instructor led workshop, your container may have already been built ahead of time, but if running this on your own, you'll need to build your deployment container.\n",
    "\n",
    "SageMaker inference supplies a variety of different built in inference containers, since `mmdetection3d` is a library with its own set of complex dependencies, we are going to use a custom hosting container. This process will take ~10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fbbbe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_NAME = 'mm3dinf'\n",
    "region = boto3.session.Session().region_name\n",
    "account = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# # if in MLR401 your container will already be built for you, you can ue the following commands to pull them down!\n",
    "\n",
    "# ! docker pull public.ecr.aws/k2j9l5n0/mm3dinf\n",
    "# ! docker tag public.ecr.aws/k2j9l5n0/mm3dinf {account}.dkr.ecr.us-east-1.amazonaws.com/mm3dinf\n",
    "# ! aws ecr get-login --no-include-email | bash\n",
    "# ! aws ecr create-repository --region {region} --repository-name {IMAGE_NAME}\n",
    "# ! docker push {account}.dkr.ecr.us-east-1.amazonaws.com/mm3dinf\n",
    "\n",
    "\n",
    "# # if running on your own uncomment out the below lines:\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "!bash ./build_and_push.sh {region} {IMAGE_NAME} latest container_inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0140ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "account = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "container = f'{account}.dkr.ecr.us-east-1.amazonaws.com/{IMAGE_NAME}'\n",
    "model_name = f'sagemaker-mm3d-{int(time.time())}'\n",
    "print(container)\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7c963",
   "metadata": {},
   "source": [
    "### Create SageMaker model\n",
    "\n",
    "Before we deploy our endpoint, we need to create a SageMaker model. A SageMaker model is different from the model we trained in the previous notebook in that it contains the information necessary to create the environment around the model so that it can be deployed as an endpoint when paired with an endpoint configuration, which will tell SageMaker what kind of instance to deploy the model to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52284db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define SageMaker model using our container and model artifact\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': model_artifact,\n",
    "        'Environment': {\n",
    "            'TS_MAX_REQUEST_SIZE': '100000000', \n",
    "            'TS_MAX_RESPONSE_SIZE': '100000000',\n",
    "            'TS_DEFAULT_RESPONSE_TIMEOUT': '1000'\n",
    "        }\n",
    "    },    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaef733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Confirm that the model was created\n",
    "create_model_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948eef39",
   "metadata": {},
   "source": [
    "## Create inference endpoint\n",
    "\n",
    "In this section, we'll create an async inference endpoint. We take the model object we created previously and pair it with an endpoint configuration that tell SageMaker how many and what type of instances to deploy the model to. In this case we also use the endpoint configuration to tell SageMaker we want to make this endpoint an asynchronous endpoint.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16dbcf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define an endpoint that uses one ml.g4dn.2xlarge instance and uses async inference\n",
    "from time import strftime,gmtime\n",
    "endpoint_config_name = f\"MM3DAsyncEndpointConfig-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g4dn.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1\n",
    "        }\n",
    "    ],\n",
    "    AsyncInferenceConfig={\n",
    "        \"OutputConfig\": {\n",
    "            \"S3OutputPath\": f\"s3://{bucket}/{prefix}/output\"\n",
    "        },\n",
    "        \"ClientConfig\": {\n",
    "            \"MaxConcurrentInvocationsPerInstance\": 1\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(f\"Created EndpointConfig: {create_endpoint_config_response['EndpointConfigArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a2789",
   "metadata": {},
   "source": [
    "Now that we have created our SageMaker model object and our endpoint configuration, we can deploy our endpoint. The deployment process takes 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115be8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy the endpoint\n",
    "endpoint_name = f\"sm-{strftime('%Y-%m-%d-%H-%M-%S', gmtime())}\"\n",
    "create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n",
    "print(f\"Creating Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48137e5d",
   "metadata": {},
   "source": [
    "Running the following cell will launch a waiter that will alert us when our endpoint has been successfully deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174acd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait for the endpoint to enter service\n",
    "waiter = boto3.client('sagemaker').get_waiter('endpoint_in_service')\n",
    "print(\"Waiting for endpoint to create...\")\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Endpoint Status: {resp['EndpointStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85bf9df",
   "metadata": {},
   "source": [
    "## Test the endpoint\n",
    "\n",
    "Next we'll upload an A2D2 point cloud file and test the inference endpoint.  Since we're using async inference, the call to the endpoint doesn't return the inference instantaneously.  Rather, it returns a pointer to the S3 output location.  We can check for the presence of the results there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f007f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper method for uploading to S3\n",
    "def upload_file(input_location, prefix):\n",
    "    prefix = f\"{prefix}/input\"\n",
    "    return sm_session.upload_data(\n",
    "        input_location, \n",
    "        bucket=sm_session.default_bucket(),\n",
    "        key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d359c",
   "metadata": {},
   "source": [
    "First we upload our point cloud file to a location in S3. Let's gather the paths to our scene IDs and pick a scene and a frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b129f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "scene_ids = sorted(glob('../fsx/a2d2/camera_lidar_semantic_bboxes/2018*'))\n",
    "scene_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba9af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_root_path = \"../fsx/a2d2/\"\n",
    "scene_id = \"20180807_145028\"\n",
    "file_scene_prefix = scene_id.replace(\"_\", \"\")\n",
    "frame_id = \"000000091\"\n",
    "\n",
    "input_1_location = f\"../fsx/a2d2/camera_lidar_semantic_bboxes/{scene_id}/lidar/cam_front_center/{file_scene_prefix}_lidar_frontcenter_{frame_id}.bin\"\n",
    "input_1_s3_location = upload_file(input_1_location, prefix)\n",
    "print(input_1_s3_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a5bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Invoke endpoint using boto3 API\n",
    "response = sm_runtime.invoke_endpoint_async(\n",
    "    EndpointName=endpoint_name, \n",
    "    InputLocation=input_1_s3_location)\n",
    "output_location = response['OutputLocation']\n",
    "print(f\"OutputLocation: {output_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23681800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the output S3 location from the response\n",
    "from urllib.parse import urlparse\n",
    "parse_object = urlparse(output_location)\n",
    "parse_object.netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa8d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parse_object.path[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97aa541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the output.  \n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.download_file(parse_object.netloc, parse_object.path[1:], 'test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ad496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The output is a pickled Python object.  Convert it back to a dictionary.\n",
    "import pickle\n",
    "with open('test.pkl', 'rb') as test_f:\n",
    "    test_out = pickle.load(test_f)\n",
    "    \n",
    "# View the output\n",
    "test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4222c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from pytransform3d import plot_utils\n",
    "from pytransform3d import rotations as pr\n",
    "from pytransform3d import transformations as pt\n",
    "\n",
    "from a2d2_helpers import undistort_image, generate_color_map\n",
    "\n",
    "\n",
    "file_scene_prefix = scene_id.replace(\"_\", \"\")\n",
    "scene_path = Path(dataset_root_path) / \"camera_lidar_semantic_bboxes\" / scene_id\n",
    "frame_image_path = scene_path / \"camera\" / \"cam_front_center\" / f\"{file_scene_prefix}_camera_frontcenter_{frame_id}.png\"\n",
    "frame_lidar_path = scene_path / \"lidar\" / \"cam_front_center\" / f\"{file_scene_prefix}_lidar_frontcenter_{frame_id}.npz\"\n",
    "frame_label_path = scene_path / \"label3D\" / \"cam_front_center\" / f\"{file_scene_prefix}_label3D_frontcenter_{frame_id}.json\"\n",
    "\n",
    "# Load our frame point cloud from the compressed NPZ format, grabbing the points and reflectance corresponding\n",
    "# to those points. We'll use the reflectance to color our points in the 3D plot.\n",
    "lidar_frame = np.load(str(frame_lidar_path))\n",
    "points = lidar_frame[\"points\"]\n",
    "reflectance = lidar_frame[\"reflectance\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,10), dpi=200)\n",
    "ax_3d = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "# Plot our point cloud.\n",
    "ax_3d.scatter(points[:,0], points[:,1], points[:,2], s=0.1, c=reflectance, marker=\",\")\n",
    "\n",
    "inference_score_pairs = list(zip(test_out[\"result\"][\"boxes_3d\"], test_out[\"result\"][\"scores_3d\"]))\n",
    "inference_score_pairs = sorted(inference_score_pairs, key=lambda inf_score_pair: -inf_score_pair[1])\n",
    "\n",
    "for inference, score in inference_score_pairs[:5]:\n",
    "    x, y, z, l, w, h, yaw = inference\n",
    "    # See https://mmdetection3d.readthedocs.io/en/latest/api.html for details on mmdetection output format.\n",
    "    qw, qx, qy, qz = pr.quaternion_from_matrix(pr.matrix_from_axis_angle([0, 0, 1, np.pi / 2 - yaw]))\n",
    "    A2B = pt.transform_from_pq([x, y, z, qw, qx, qy, qz])\n",
    "    size = [w, l, h]\n",
    "\n",
    "    plot_utils.plot_box(ax_3d, size=size, A2B=A2B, alpha=0.4)\n",
    "    plot_utils.Frame(A2B).add_frame(ax_3d)\n",
    "\n",
    "# Load our frame image from the file system.\n",
    "image_frame = cv2.imread(str(frame_image_path))\n",
    "image_frame = cv2.cvtColor(image_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# Show image side by side.\n",
    "ax_2d = fig.add_subplot(1, 2, 2)\n",
    "ax_2d.imshow(image_frame)\n",
    "ax_2d.set_title(\"2D Image\")\n",
    "\n",
    "\n",
    "limit_m = 10\n",
    "translation = [25, 0, 0]\n",
    "ax_3d.set_xlim((-limit_m + translation[0], limit_m + translation[0]))\n",
    "ax_3d.set_ylim((-limit_m + translation[1], limit_m + translation[1]))\n",
    "ax_3d.set_zlim((-limit_m + translation[2], limit_m + translation[2]))\n",
    "ax_3d.set_xlabel(\"X\")\n",
    "ax_3d.set_ylabel(\"Y\")\n",
    "ax_3d.set_zlabel(\"Z\")\n",
    "ax_3d.set_title('Point cloud with predicted 3D boxes')\n",
    "ax_3d.view_init(5, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71723515",
   "metadata": {},
   "source": [
    "## Local mode testing\n",
    "\n",
    "If you want to test inference in local mode, you can use this section of the notebook.  You'll need to edit the location of the input point cloud file.  Also note that we specify the SHA of the current version of the container image, so that the local mode endpoint knows when we have a new image to test. In our case we are just going to use the latest version of the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a09a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT CONTAINER IMAGE URL AND SHA IN THIS CELL\n",
    "\n",
    "container = f'{account}.dkr.ecr.us-east-1.amazonaws.com/mm3dinf'\n",
    "container_sha = ''\n",
    "# you can optionally add a SHA specification to the container to use a specific version, we will just use the latest version\n",
    "# simply replace f\"{container}\" with f\"{container}@{container_sha}\"\n",
    "s_model = sagemaker.model.Model(f\"{container}\", model_data=model_artifact, role=role, predictor_cls=sagemaker.predictor.Predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "instance_type = \"local_gpu\" # if torch.cuda.is_available() else \"local\"\n",
    "\n",
    "# Deploy a new local endpoint\n",
    "predictor = s_model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdfa2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is using the point cloud location we gave to our asynchronous endpoint above, you can change it to test different files\n",
    "\n",
    "with open(input_1_location, 'rb') as in_file:\n",
    "    d = in_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction locally\n",
    "r = predictor.predict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialize the output\n",
    "import pickle\n",
    "rr = pickle.loads(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d378316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check our results\n",
    "rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46571bbe",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Make sure to delete unused endpoints, especially if you are running this in your own account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae984c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.delete_endpoint(\n",
    "    EndpointName=endpoint_name\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d390d0d9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've now learned how to use Amazon SageMaker for end to end 3D machine learning! We hope you enjoyed this workshop, now it's your turn to apply what you've learned here to your own machine learning workflows!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
