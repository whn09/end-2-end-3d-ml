{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec93210f",
   "metadata": {},
   "source": [
    "# A2D2 Visualization and Labeling\n",
    "\n",
    "![](display_images/remars_logo.png)\n",
    "\n",
    "Welcome to the End to End 3D ML workshop on [SageMaker](https://aws.amazon.com/sagemaker/). In this workshop we'll be walking through\n",
    "how to parse a 3D dataset, visualize/label it, train a model on it, then deploy our model as a SageMaker endpoint\n",
    "for inference. For the workshop, we'll be working with the [A2D2 driving dataset](https://www.a2d2.audi/a2d2/en.html),\n",
    "which provides both 3D lidar and 2D camera data for a variety of driving scenes.\n",
    "\n",
    "This workshop is split across three notebooks:\n",
    "\n",
    "1. 1_vizualization.ipynb: Download and visualize the dataset.\n",
    "2. 2_sagemaker_training.ipynb: train a 3D object detection model on the dataset.\n",
    "3. 3_inference_byom.ipynb: Deploy an asynchronous SageMaker endpoint and run inference on the trained model.\n",
    "\n",
    "In this notebook we'll look at how to interpret the A2D2 dataset structure\n",
    "and how to add additional labels to the dataset.\n",
    "\n",
    "#### 1) Download the A2D2 dataset\n",
    "\n",
    "First we'll download the dataset to our [Amazon FSx filesystem](https://aws.amazon.com/fsx/lustre/) for fast training.\n",
    "\n",
    "#### 2) Visualize the dataset in 2D and 3D\n",
    "\n",
    "In this section we'll understand the dataset contents and visualize them in 2D and 3D.\n",
    "\n",
    "#### 3) Verify the dataset's sensor fusion for labeling\n",
    "\n",
    "We'll then project points from 3D to 2D to verify we're interpreting the positions and orientations of sensors correctly before sending the data for labeling in SageMaker Ground Truth.\n",
    "\n",
    "#### 4) Convert the dataset to SageMaker Ground Truth format and kickoff a labeling job\n",
    "\n",
    "Finally we'll convert a scene to a SageMaker Ground Truth input manifest that can be labeled by a worker. *Note that we won't be able to start\n",
    "the SageMaker Ground Truth labeling job during the workshop, however this step can be configured and run in your own account*.\n",
    "\n",
    "Note: **Please use conda_python3 as the kernel for this notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18868839",
   "metadata": {},
   "source": [
    "Let's start by installing a visualization library and importing a few others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6dd29",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pytransform3d opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7d6b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pytransform3d import rotations as pr\n",
    "from pytransform3d import transformations as pt\n",
    "from pytransform3d.transform_manager import TransformManager\n",
    "from pytransform3d import plot_utils\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.session.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "dataset_root_path = Path(\"../fsx/a2d2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72d728",
   "metadata": {},
   "source": [
    "## Dataset Download\n",
    "\n",
    "We'll download the object detection subset of A2D2 to your local notebook instance\n",
    "for visualization and inspection before training. This process will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08afd1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!mkdir -p {str(dataset_root_path)}\n",
    "!aws s3 cp s3://aev-autonomous-driving-dataset/cams_lidars.json {str(dataset_root_path)}    \n",
    "!aws s3 cp s3://aws-tc-largeobjects/DEV-AWS-MO-Nvidia/a2d2_gt_database.tar.gz {str(dataset_root_path)}\n",
    "!aws s3 cp s3://aws-tc-largeobjects/DEV-AWS-MO-Nvidia/camera_lidar_semantic_bboxes.tar {str(dataset_root_path)}\n",
    "!tar -C {str(dataset_root_path)} -xf ../fsx/a2d2/camera_lidar_semantic_bboxes.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e4ad5",
   "metadata": {},
   "source": [
    "## Downloaded Files\n",
    "\n",
    "We've downloaded the 3D object detection dataset from A2D2 into the directory `a2d2/camera_lidar_semantic_bboxes`. You can read more about this dataset in the [README](https://aev-autonomous-driving-dataset.s3.eu-central-1.amazonaws.com/README-3DBoxes.txt).\n",
    "\n",
    "\n",
    "Under the path `camera_lidar_semantic_bboxes` we find a flat list of scenes.\n",
    "A scene is a short recording of sensor data from our autonomous vehicle. A2D2 provides 18 of these scenes for us to train on, all identified\n",
    "by unique dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175b398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls {str(dataset_root_path)}/camera_lidar_semantic_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3108b7",
   "metadata": {},
   "source": [
    "Each scene contains 2D camera data, 2D labels, 3D cuboid annotations, and 3D point clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2bb8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls {str(dataset_root_path)}/camera_lidar_semantic_bboxes/20180807_145028/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94297ee6",
   "metadata": {},
   "source": [
    "We're interested specifically in the `camera`, `label3D`, and `lidar` folders as they contain\n",
    "the data we'll be using for training.\n",
    "\n",
    "\n",
    "Camera images are stored under the `camera` path as `png` files.\n",
    "\n",
    "\n",
    "`camera/cam_front_center/<scene_id>_cam_front_center_<ts>.png`\n",
    "\n",
    "\n",
    "3D point clouds (arrays of X,Y,Z points) are provided inside these compressed numpy structs.\n",
    "\n",
    "\n",
    "`lidar/cam_front_center/<scene_id>_cam_front_center_<ts>.npz`\n",
    "\n",
    "\n",
    "Finally our the labels we'll use for training are stored within the `label3D` folder inside json files.\n",
    "\n",
    "\n",
    "`label3D/cam_front_center/<scene_id>_cam_front_center_<ts>.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd588ff",
   "metadata": {},
   "source": [
    "## A2D2 Sensor Setup\n",
    "\n",
    "In addition to our dataset, we also downloaded `cams_lidars.json` which contains the translation and orientation of each sensor relative to the vehicle's\n",
    "[coordinate frame](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud-sensor-fusion-details.html#sms-point-cloud-world-coordinate-system).\n",
    "\n",
    "\n",
    "We'll refer to this translation and orientation of each sensor as the sensor \"pose.\" A sensor's pose is nicely described by a single 4x4 matrix known as \n",
    "the rigid body [transformation matrix](https://en.wikipedia.org/wiki/Transformation_matrix). If you have a 3x3 rotation matrix `R` and a translation vector `T`, the transformation matrix is simply `[R T; 0 0 0 1]`. In the context of A2D2, we'll use this transformation matrix to convert points from a sensor's coordinate frame to the vehicle coordinate frame. For more details, please refer to the [A2D2 data tutorial](https://www.a2d2.audi/a2d2/en/tutorial.html).\n",
    "\n",
    "Let's print out the transform matrix of the front center camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a7b903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from a2d2_helpers import get_transform_to_global\n",
    "\n",
    "with open(dataset_root_path / \"cams_lidars.json\") as f:\n",
    "    cams_lidars = json.load(f)\n",
    "    \n",
    "get_transform_to_global(cams_lidars[\"cameras\"][\"front_center\"][\"view\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d075282",
   "metadata": {},
   "source": [
    "In A2D2 (and many robotics datasets) the vehicle's coordinate frame is defined as some static point in the center of the vehicle, with the\n",
    "x-axis in the direction of travel of the vehicle, the y-axis pointing out the left side of the vehicle, and the z-axis\n",
    "pointing up through the roof of the vehicle.\n",
    "\n",
    "For example, a point (X, Y, Z) = (5, 2, 1) refers to a point 5 meters ahead of our vehicle, 2 meters to the left, and 1 meter above us.\n",
    "\n",
    "\n",
    "The A2D2 capture vehicle contains multiple lidars and cameras, we can see the sensor setup by going to the link below.\n",
    "\n",
    "https://www.a2d2.audi/a2d2/en/sensor-setup.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a31b919",
   "metadata": {},
   "source": [
    "### PyTransform3D and Sensor Visualization\n",
    "\n",
    "Now that we have a rough understanding of what our coordinate system looks like and where the front center camera is located\n",
    "on the vehicle, we can try to plot the sensor locations ourselves and make sure the plot matches the above diagram.\n",
    "\n",
    "[PyTransform3D](https://rock-learning.github.io/pytransform3d/) is a great library for visualizing and managing 3D transforms. It is very useful for verifying that we are interpreting matrices correctly and didn't flip a sign or invert a transformation accidentally!\n",
    "\n",
    "\n",
    "In particular, PyTransform3D provides a `TransformManager` object where we can store the transform matrix describing how to map points from\n",
    "one coordinate frame to another. If I have two lidar sensors that both measure points relative to the sensor location, I can simply define the transformation\n",
    "matrix between them, then convert measurements from one lidar to the coordinate frame of the other lidar.\n",
    "\n",
    "\n",
    "Ensuring you merge sensor measurements appropriately, especially when there are both translations and rotations between your sensors is\n",
    "hard work! Fortunately, all merging of point clouds and projection of points into front camera relative space has already been done by the A2D2 object detection dataset, but we'll still use PyTransform3D to inspect the calibration data for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ca034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_transform_manager(cam_lidar):\n",
    "    \"\"\"Converts from the A2D2 cams_lidars.json format to a TransformManager we can use\n",
    "    for visualization\"\"\"\n",
    "    tm = TransformManager()\n",
    "\n",
    "    camera_frames = []\n",
    "    for camera in cams_lidars['cameras']:\n",
    "        transform = get_transform_to_global(cams_lidars['cameras'][camera]['view'])\n",
    "        sensor_name = f\"cam_{camera}\"\n",
    "        tm.add_transform(sensor_name, \"vehicle\", transform)\n",
    "        camera_frames.append(sensor_name)\n",
    "\n",
    "    lidar_frames = []\n",
    "    for lidar in cams_lidars['lidars']:\n",
    "        transform = get_transform_to_global(cams_lidars['lidars'][lidar]['view'])\n",
    "        sensor_name = f\"lid_{lidar}\"\n",
    "        tm.add_transform(sensor_name, \"vehicle\", transform)\n",
    "        lidar_frames.append(sensor_name)\n",
    "        \n",
    "    return tm, camera_frames, lidar_frames\n",
    "\n",
    "transform_manager, camera_frames, lidar_frames = gen_transform_manager(cams_lidars)\n",
    "# Construct two lists of sensors to visualize our lidar sensors and camera sensors separately.\n",
    "veh_and_camera = camera_frames + [\"vehicle\"]\n",
    "veh_and_lidar = lidar_frames + [\"vehicle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5de38",
   "metadata": {},
   "source": [
    "### Visualize sensor locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041582f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize the locations of the cameras with respect to the vehicle center.\n",
    "# Note that (R, G, B) corresponds to (X, Y, Z) of the coordinate frame.\n",
    "fig = plt.figure(figsize=(10,10), dpi=200)\n",
    "cameras_ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "lidars_ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax = transform_manager.plot_frames_in(\"vehicle\", ax=cameras_ax, s=0.5, show_name=True, whitelist=veh_and_camera)\n",
    "cameras_ax.set_title(\"Camera sensor coordinate frames\")\n",
    "\n",
    "limit_m = 2\n",
    "translation = [2, 0, 0]\n",
    "\n",
    "\n",
    "# Visualize the locations of the cameras with respect to the vehicle center.\n",
    "fig = plt.figure(figsize=(10,10), dpi=200)\n",
    "ax = transform_manager.plot_frames_in(\"vehicle\", ax=lidars_ax, s=0.5, show_name=True, whitelist=veh_and_lidar)\n",
    "lidars_ax.set_title(\"Lidar sensor coordinate frames\")\n",
    "\n",
    "for ax in (cameras_ax, lidars_ax):\n",
    "    ax.set_xlim((-limit_m + translation[0], limit_m + translation[0]))\n",
    "    ax.set_ylim((-limit_m + translation[1], limit_m + translation[1]))\n",
    "    ax.set_zlim((-limit_m + translation[2], limit_m + translation[2]))\n",
    "    ax.view_init(45, 270) # You can play with this value to plot from different perspectives.\n",
    "    ax.set_xlabel(\"X\"); ax.set_ylabel(\"Y\"); ax.set_zlabel(\"Z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30de01",
   "metadata": {},
   "source": [
    "## Plotting 3D Labels and 2D Projections\n",
    "\n",
    "Now that we know we're reading the sensor poses correctly based on the above plots,\n",
    "lets overlay our point cloud and cuboid labels in a 3D plot.\n",
    "\n",
    "A2D2 has already done all the conversions of your input dataset points so that they are relative to the\n",
    "`cam_front_center` camera. It has also removed all points not in the cameras field of view. Let's treat\n",
    "`cam_front_center` as our origin coordinate frame, and plot all points and labels relative to that coordinate frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520ce2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from a2d2_helpers import undistort_image, a2d2_box_to_transform_matrix_and_size, draw_2d_bounding_box, generate_color_map\n",
    "\n",
    "# Pick a random scene from the dataset. 3D datasets are typically organized\n",
    "# as a series of sequential timesteps where both 3D lidar data and 2D camera data\n",
    "# are available.\n",
    "scene_id = \"20180807_145028\"\n",
    "frame_id = \"000000091\"\n",
    "file_scene_prefix = scene_id.replace(\"_\", \"\")\n",
    "scene_path = Path(dataset_root_path) / \"camera_lidar_semantic_bboxes\" / scene_id\n",
    "frame_image_path = scene_path / \"camera\" / \"cam_front_center\" / f\"{file_scene_prefix}_camera_frontcenter_{frame_id}.png\"\n",
    "frame_lidar_path = scene_path / \"lidar\" / \"cam_front_center\" / f\"{file_scene_prefix}_lidar_frontcenter_{frame_id}.npz\"\n",
    "frame_label_path = scene_path / \"label3D\" / \"cam_front_center\" / f\"{file_scene_prefix}_label3D_frontcenter_{frame_id}.json\"\n",
    "\n",
    "# Load our frame labels, we'll use these to draw in our 3D view and to draw the 2D projections over the actual camera.\n",
    "with open(frame_label_path) as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Load our frame image from the file system.\n",
    "image_frame = cv2.imread(str(frame_image_path))\n",
    "image_frame = cv2.cvtColor(image_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Load our frame point cloud from the compressed NPZ format, grabbing the points and reflectance corresponding\n",
    "# to those points. We'll use the reflectance to color our points in the 3D plot.\n",
    "lidar_frame = np.load(str(frame_lidar_path))\n",
    "points = lidar_frame[\"points\"]\n",
    "reflectance = lidar_frame[\"reflectance\"]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10), dpi=200)\n",
    "ax_3d = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "transform_manager.plot_frames_in(\"cam_front_center\", ax=ax_3d, s=0.5, show_name=True, whitelist=[\"cam_front_center\"])\n",
    "\n",
    "# Plot our point cloud.\n",
    "ax_3d.scatter(points[:,0], points[:,1], points[:,2], c=lidar_frame[\"reflectance\"], s=0.1, marker=\",\")\n",
    "\n",
    "# Plot our 3d labels projected to 2d (part of A2D2 dataset)\n",
    "for box_id, values in labels.items():\n",
    "    A2B, size = a2d2_box_to_transform_matrix_and_size(values)\n",
    "    plot_utils.plot_box(ax_3d, size=size, A2B=A2B, alpha=0.4)\n",
    "    plot_utils.Frame(A2B).add_frame(ax_3d)\n",
    "\n",
    "# Image undistortion corrects for multiple camera imperfections that make a camera\n",
    "# not behave like an ideal pinhole camera model. It's important we undistort here\n",
    "# since the 2D labels A2D2 provides correspond to the undistorted image, not\n",
    "# the distorted version.\n",
    "undistorted_image = undistort_image(image_frame, \"front_center\", cams_lidars)\n",
    "color_map = generate_color_map(labels)\n",
    "for box_id, box in labels.items():\n",
    "    draw_2d_bounding_box(undistorted_image, box[\"2d_bbox\"], box[\"class\"], color_map[box[\"class\"]])\n",
    "\n",
    "    \n",
    "# Show image side by side.\n",
    "ax_2d = fig.add_subplot(1, 2, 2)\n",
    "ax_2d.imshow(undistorted_image)\n",
    "ax_2d.set_title(\"Image with projected labels overlayed\")\n",
    "\n",
    "\n",
    "limit_m = 10\n",
    "translation = [25, 0, 0]\n",
    "ax_3d.set_xlim((-limit_m + translation[0], limit_m + translation[0]))\n",
    "ax_3d.set_ylim((-limit_m + translation[1], limit_m + translation[1]))\n",
    "ax_3d.set_zlim((-limit_m + translation[2], limit_m + translation[2]))\n",
    "ax_3d.set_title(\"Point cloud view using point distance for coloration\")\n",
    "ax_3d.view_init(5, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd66fd6-9a45-440d-be1c-c0c435a6d41f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lidar_frame, lidar_frame[\"points\"].shape, lidar_frame[\"reflectance\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063bb2d-b437-4575-b9ff-8f8826b781bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lidar_frame[\"points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92baf48-73a2-45f2-b36b-7d805c135318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lidar_frame[\"reflectance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c563a3",
   "metadata": {},
   "source": [
    "## Projection from 3D to 2D\n",
    "\n",
    "For both labeling and training mixed-modality models, you need to provide your model\n",
    "with details of how to map from 3D points to 2D points in camera frame.\n",
    "\n",
    "\n",
    "This transformation from 3D to 2D is described by both the camera extrinsics (the position\n",
    "and orientation of the camera in 3D space) as well as the [camera matrix](https://en.wikipedia.org/wiki/Camera_matrix) (the mapping\n",
    "from 3D coordinate in camera coordinate system to a 2D pixel value).\n",
    "\n",
    "\n",
    "This projection is done in four steps:\n",
    "\n",
    "1. Transform points into camera coordinate system\n",
    "2. Multiply each point by our camera matrix (and normalize)\n",
    "3. Drop points outside of camera range\n",
    "4. Color points by distance and overlay on camera image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310bde57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from a2d2_helpers import x_forward_to_z_foward\n",
    "\n",
    "# Let's overlay the point cloud on the image frame using the camera extrinsics and intrinsics.\n",
    "# This is useful to get right since it can help human labelers better visualize which points belong\n",
    "# to which object classes when annotating.\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=200)\n",
    "\n",
    "undistorted_image = undistort_image(image_frame, \"front_center\", cams_lidars)\n",
    "points = lidar_frame[\"points\"]\n",
    "# Convert to homogenous coordinate system.\n",
    "points = np.append(points, np.ones((points.shape[0], 1)), axis=1)\n",
    "\n",
    "color_map = generate_color_map(labels)\n",
    "for box_id, box in labels.items():\n",
    "    draw_2d_bounding_box(undistorted_image, box[\"2d_bbox\"], box[\"class\"], color_map[box[\"class\"]])\n",
    "\n",
    "# Here we get our camera matrix that maps from 3D points in the camera coordinate system\n",
    "# and projects them to 2D pixel locations.\n",
    "camera_matrix = np.array(cams_lidars['cameras']['front_center']['CamMatrix']).reshape(3, 3)\n",
    "camera_matrix = np.append(camera_matrix, np.zeros((camera_matrix.shape[0], 1)), axis=1)\n",
    "\n",
    "# The cam_front_center_ext coordinate system is essentially just the cam_front_center coordinate system,\n",
    "# but with the z-axis of the coordinate system facing the direction of the camera rather than\n",
    "# the x-axis.\n",
    "transform_manager.add_transform(\n",
    "    \"cam_front_center_ext\",\n",
    "    \"vehicle\",\n",
    "    x_forward_to_z_foward(\n",
    "        transform_manager.get_transform(\"cam_front_center\", \"vehicle\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert points from the camera pose frame to the camera extrinsics coordinate frame,\n",
    "# that has z-forward, x-right, and y-down.\n",
    "points_in_camera_ext = points @ transform_manager.get_transform(\"cam_front_center_ext\", \"cam_front_center\")\n",
    "\n",
    "# Now use the camera matrix to convert points from extrinsics coordinate frame to 2D image plane,\n",
    "# https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/EPSRC_SSAZ/node3.html\n",
    "projected_points = (camera_matrix @ points_in_camera_ext.T).T\n",
    "projected_points[:, 0] /= projected_points[:, 2]\n",
    "projected_points[:, 1] /= projected_points[:, 2]\n",
    "points_in_image = projected_points\n",
    "\n",
    "# Drop points outside of image viewport and behind camera plane\n",
    "indices_to_drop = np.where((points_in_image[:, 0] < 0) |\n",
    "                           (points_in_image[:, 1] < 0) |\n",
    "                           (points_in_image[:, 0] > undistorted_image.shape[1]) | \n",
    "                           (points_in_image[:, 1] > undistorted_image.shape[0]) | \n",
    "                           (points_in_camera_ext[:, 2] < 0))[0]\n",
    "\n",
    "points_in_image = np.delete(points_in_image, indices_to_drop, axis=0)\n",
    "\n",
    "# Color based on z-distance from camera plane\n",
    "colors = points_in_camera_ext[:,2]\n",
    "colors = np.sqrt(colors)\n",
    "colors = np.expand_dims(colors/max(colors), 1)\n",
    "colors = np.hstack((colors, 1-colors, colors-colors))\n",
    "\n",
    "plt.imshow(undistorted_image, cmap='gray', extent=(0, undistorted_image.shape[1], undistorted_image.shape[0], 0)) # left, right, bottom, top\n",
    "plt.title(\"Point cloud overlaid on camera frame, colored by distance\")\n",
    "\n",
    "\n",
    "res = plt.scatter(points_in_image[:,0], points_in_image[:,1], s=0.5, alpha=0.5, c=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35acaf",
   "metadata": {},
   "source": [
    "# Conversion to SageMaker Ground Truth\n",
    "\n",
    "<span style=\"color:red;font-size:22.0pt\"> The SageMaker Ground Truth job launch will not be possible inside of an AWS Instructor led workshop, however you can run the cells in a personal account.</span>\n",
    "\n",
    "<span style=\"color:red;font-size:18.0pt\"> If in an AWS instructor led workshop you can run the cells up to \"Create a labeling workteam\" to generate the files you would need in order to launch a job.</span>\n",
    "\n",
    "Now that we have visualized our data, and shown we can perform the projection from point cloud to image frame,\n",
    "we can confidently convert our point clouds into [SageMaker Ground Truth's 3D format](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud-multi-frame-input-data.html) to verify and adjust our labels.\n",
    "\n",
    "\n",
    "We'll use the multi-frame [object tracking modality](TODO) to let us see all the point cloud frames in the same labeling task,\n",
    "however since we are ultimately only doing object detection, we'll ignore object tracks ids before training.\n",
    "\n",
    "(Image from Amazon SageMaker Ground Truth Documentation - https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud.html)\n",
    "![](https://docs.aws.amazon.com/sagemaker/latest/dg/images/pointcloud/gifs/semantic_seg/ss_paint_sf.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0e536",
   "metadata": {},
   "source": [
    "## Generate a Sequence File\n",
    "\n",
    "We'll convert our dataset to Ground Truth compatible formats and generate a sequence file that is used as input to SageMaker Ground Truth for labeling.\n",
    "\n",
    "A sequence specifies a temporal series of point cloud frames. When a task is created using a sequence file, all point cloud frames in the sequence are sent to a worker to label. Your input manifest file will contain a single sequence per line. To learn more about the sequence input manifest format, see [Create a Point Cloud Frame Sequence Input Manifest](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud-multi-frame-input-data.html).\n",
    "\n",
    "If you want to use this script to create a frame input manifest file, which is required for 3D point cloud object tracking and semantic segmentation labeling jobs, you can modify the for-loop in the function convert_to_gt to produce the required content for source-ref-metadata. To learn more about the frame input manifest format (different from the above sequence manifest format), see [Create a Point Cloud Frame Input Manifest File](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud-single-frame-input-data.html).\n",
    "\n",
    "Now, let's download the script and run it on the A2D2 dataset to process the data you'll use for your labeling job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac7187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os.path import splitext\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def a2d2_scene_to_smgt_sequence_and_seq_label(a2d2_scene_path, smgt_assets_path, sequence_s3_uri, num_frames=30):\n",
    "    \"\"\"\n",
    "    This function converts from an A2D2 scene to SageMaker Ground Truth format.\n",
    "    \n",
    "    https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud-multi-frame-input-data.html\n",
    "    \"\"\"\n",
    "    if not sequence_s3_uri.endswith(\"/\"):\n",
    "        sequence_s3_uri += \"/\"\n",
    "\n",
    "    scene_id = a2d2_scene_path.rsplit(\"/\", 1)[-1]\n",
    "    \n",
    "    smgt_assets_path = Path(smgt_assets_path)\n",
    "    smgt_assets_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get a list of all lidar, camera, and 3D labels for the scene.\n",
    "    lidar_frames = [Path(pth) for pth in sorted(glob.glob(str(Path(a2d2_scene_path) / \"lidar\" / \"cam_front_center\" / \"*.npz\")))]\n",
    "    camera_frames = [Path(pth) for pth in sorted(glob.glob(str(Path(a2d2_scene_path) / \"camera\" / \"cam_front_center\" / \"*.png\")))]\n",
    "    label_frames = [Path(pth) for pth in sorted(glob.glob(str(Path(a2d2_scene_path) / \"label3D\" / \"cam_front_center\" / \"*.json\")))]\n",
    "    \n",
    "    # For each frame (a point in time with a synchronized lidar and camera reading), convert the frame to SageMaker Ground Truth format.\n",
    "    frames = []\n",
    "    for i, (lidar_frame, camera_frame) in enumerate(zip(lidar_frames, camera_frames)):\n",
    "        if i >= num_frames:\n",
    "            continue\n",
    "        \n",
    "        # Parse the timestamp from each file path and verify they match.\n",
    "        ts_lidar = lidar_frame.stem.rsplit(\"_\", 1)[-1]\n",
    "        ts_camera = camera_frame.stem.rsplit(\"_\", 1)[-1]\n",
    "        assert ts_lidar == ts_camera\n",
    "        \n",
    "        # Convert points from a numpy array to an ascii format.\n",
    "        # See - https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud-raw-data-types.html for more details.\n",
    "        points_txt_path = smgt_assets_path / (lidar_frame.stem + \".txt\")\n",
    "        if not points_txt_path.exists():\n",
    "            lidar_frame_contents = np.load(lidar_frame)\n",
    "            points = lidar_frame_contents[\"points\"]\n",
    "            np.savetxt(points_txt_path, points)\n",
    "            \n",
    "        # Perform undistortion of images using A2D2 helper `undistort_image`.\n",
    "        # For Amazon SageMaker Ground Truth to correctly align 3D points with our 2D images, our images\n",
    "        # need to be undisorted before ingestion, or undistortion parameters can be provided to SageMaker\n",
    "        # Ground Truth directly. Here we perform undistortion ourselves using A2D2's helper function\n",
    "        # `undistort_image`.\n",
    "        camera_path = smgt_assets_path / f\"undistort_{camera_frame.name}\"\n",
    "        image_frame = cv2.imread(str(camera_frame))\n",
    "        undistorted_image = undistort_image(image_frame, \"front_center\", cams_lidars)\n",
    "        cv2.imwrite(str(camera_path), undistorted_image)\n",
    "\n",
    "        # Get the position and heading of the front camera as a single array.\n",
    "        # The format of pq = [x, y, z, qw, qx, qy, qz] where (x, y, z) refer to object\n",
    "        # position while the remaining (qw, qx, qy, qz) correspond to camera heading.\n",
    "        camera_transform = transform_manager.get_transform(\"cam_front_center_ext\", \"vehicle\")\n",
    "        pq = pt.pq_from_transform(camera_transform)\n",
    "        \n",
    "        frames.append({\n",
    "            \"frame-no\": i,\n",
    "            \"unix-timestamp\": int(ts_lidar) / 1000.,\n",
    "            \"frame\": points_txt_path.name,\n",
    "            \"format\": \"text/xyz\",\n",
    "            \"ego-vehicle-pose\": {\n",
    "                # We'll ignore pose since A2D2 doesn't directly provide it in their object detection\n",
    "                # dataset, however providing pose can improve interpolation. See Amazon SageMaker Ground Truth's\n",
    "                # product documentation for more details: https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud-sensor-fusion-details.html\n",
    "                \"position\": {\n",
    "                    \"x\": 0,\n",
    "                    \"y\": 0,\n",
    "                    \"z\": 0,\n",
    "                },\n",
    "                \"heading\": {\n",
    "                    \"qw\": 1,\n",
    "                    \"qx\": 0,\n",
    "                    \"qy\": 0,\n",
    "                    \"qz\": 0,\n",
    "                },\n",
    "            },\n",
    "            \"images\": [{\n",
    "                \"image-path\": camera_path.name,\n",
    "                \"unix-timestamp\": int(ts_lidar) / 1000.,\n",
    "                \"fx\": camera_matrix[0, 0],\n",
    "                \"fy\": camera_matrix[1, 1],\n",
    "                \"cx\": camera_matrix[0, 2],\n",
    "                \"cy\": camera_matrix[1, 2],\n",
    "                \"position\": {\n",
    "                    \"x\": pq[0],\n",
    "                    \"y\": pq[1],\n",
    "                    \"z\": pq[2],\n",
    "                },\n",
    "                \"heading\": {\n",
    "                    \"qw\": pq[3],\n",
    "                    \"qx\": pq[4],\n",
    "                    \"qy\": pq[5],\n",
    "                    \"qz\": pq[6],\n",
    "                },\n",
    "                \"camera-model\": \"pinhole\",\n",
    "            }],\n",
    "        })\n",
    "        \n",
    "    source_seq = {\n",
    "      \"seq-no\": 1,\n",
    "      \"prefix\": sequence_s3_uri,\n",
    "      \"number-of-frames\": len(frames),\n",
    "      \"frames\": frames,\n",
    "    }\n",
    "    with open(smgt_assets_path / \"seq.json\", \"w\") as f:\n",
    "        json.dump(source_seq, f)\n",
    "        \n",
    "    with open(smgt_assets_path / \"single.manifest\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"source-ref\": sequence_s3_uri + \"seq.json\"\n",
    "        }, f)\n",
    "    \n",
    "        \n",
    "scene_id = \"20180807_145028\"\n",
    "a2d2_scene_to_smgt_sequence_and_seq_label(\n",
    "    a2d2_scene_path=f\"{dataset_root_path}/camera_lidar_semantic_bboxes/20180807_145028\",\n",
    "    smgt_assets_path=f\"smgt_converted/{scene_id}\",\n",
    "    sequence_s3_uri=f\"s3://{bucket}/a2d2_smgt/{scene_id}_out\",\n",
    "    num_frames=3,\n",
    ")\n",
    "\n",
    "scene_upload_s3_prefix = f\"s3://{bucket}/a2d2_smgt/{scene_id}_out\"\n",
    "!aws s3 sync smgt_converted/20180807_145028 {scene_upload_s3_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda21428",
   "metadata": {},
   "source": [
    "Now we can inspect a sequence file we created, note that it follows the SageMaker Ground Truth input format specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff8e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp {scene_upload_s3_prefix}/seq.json /tmp/seq.json\n",
    "!cat /tmp/seq.json | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80273a",
   "metadata": {},
   "source": [
    "## Identify Resources for Labeling Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51727e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_type = \"3DPointCloudObjectTracking\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbda64",
   "metadata": {},
   "source": [
    "### Specify Human Task UI ARN\n",
    "\n",
    "The following will be used to identify the HumanTaskUiArn. When you create a 3D point cloud labeling job, \n",
    "Ground Truth provides a worker UI that is specific to your task type. You can learn more about this UI\n",
    "and the assistive labeling tools that Ground Truth provides for Object Tracking on the Object Tracking task type page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9130875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_task_ui_arn = (\n",
    "    f\"arn:aws:sagemaker:{region}:394669845002:human-task-ui/PointCloudObjectTracking\"\n",
    ")\n",
    "human_task_ui_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14e16a",
   "metadata": {},
   "source": [
    "### Label Category Configuration File\n",
    "\n",
    "Your label category configuration file is used to specify labels, or classes, for your labeling job.\n",
    "\n",
    "When you use the object detection or object tracking task types, you can also include label attributes in your label category configuration file. Workers can assign one or more attributes you provide to annotations to give more information about that object. For example, you may want to use the attribute occluded to have workers identify when an object is partially obstructed.\n",
    "\n",
    "Let's look at an example of the label category configuration file for an object detection or object tracking labeling job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd00e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://aws-ml-blog.s3.amazonaws.com/artifacts/gt-point-cloud-demos/label-category-config/label-category.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47e661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"label-category.json\", \"r\") as j:\n",
    "    json_data = json.load(j)\n",
    "    print(\n",
    "        \"\\nA label category configuration file: \\n\\n\",\n",
    "        json.dumps(json_data, indent=4, sort_keys=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f295e8b6",
   "metadata": {},
   "source": [
    "Now let's send this label category file to S3 where SageMaker Ground Truth can access it. The next cell defines the pre and post annotation lambdas used for the labeling task. These are predefined lambda functions, we simply need to specify the right one according the region we are launching our labeling job in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9928d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp label-category.json s3://{bucket}/label-category.json\n",
    "label_category_config_s3uri = f's3://{bucket}/label-category.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac09bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ac_arn_map = {\n",
    "    \"us-west-2\": \"081040173940\",\n",
    "    \"us-east-1\": \"432418664414\",\n",
    "    \"us-east-2\": \"266458841044\",\n",
    "    \"eu-west-1\": \"568282634449\",\n",
    "    \"ap-northeast-1\": \"477331159723\",\n",
    "}\n",
    "\n",
    "prehuman_arn = \"arn:aws:lambda:{}:{}:function:PRE-{}\".format(region, ac_arn_map[region], task_type)\n",
    "acs_arn = \"arn:aws:lambda:{}:{}:function:ACS-{}\".format(region, ac_arn_map[region], task_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f22091",
   "metadata": {},
   "source": [
    "### Create a labeling workteam\n",
    "\n",
    "You will create some of the resources you need to launch a Ground Truth labeling job in this notebook. You must create the following resources before executing this notebook:\n",
    "\n",
    "* A work team. A work team is a group of workers that complete labeling tasks. If you want to preview the worker UI and execute the labeling task you will need to create a private work team, add yourself as a worker to this team, and provide the work team ARN below. This [GIF](images/create-workteam-loop.gif) demonstrates how to quickly create a private work team on the Amazon SageMaker console. If you do not want to use a private or vendor work team ARN, set `private_work_team` to `False` to use the Amazon Mechanical Turk workforce. To learn more about private, vendor, and Amazon Mechanical Turk workforces, see [Create and Manage Workforces\n",
    "](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "workteam_arn = 'arn:aws:sagemaker:us-east-1:209419068016:workteam/private-crowd/workshop-team'\n",
    "\n",
    "print(f'This notebook will use the work team ARN: {workteam_arn}')\n",
    "\n",
    "# Make sure workteam arn is populated if private work team is chosen\n",
    "assert (workteam_arn != '<<ADD WORK TEAM ARN HERE>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe2dba",
   "metadata": {},
   "source": [
    "### Launch the labeling job\n",
    "\n",
    "<span style=\"color:red;font-size:16.0pt\"> If you are in an AWS instructor led workshop you will not be able to run this section, however, if you are running this in your own account you can uncomment the below cells and launch a labeling job.</span>\n",
    "\n",
    "In the following cell you configure the labeling job request. To learn more about the parameters used to configure a labeling job, see [Create a Labeling Job (API)](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-create-labeling-job-api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e69fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sagemaker_cl = boto3.client(\"sagemaker\")\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "job_name = f\"a2d2-{now}\"\n",
    "\n",
    "request = {\n",
    "    \"LabelingJobName\": job_name,\n",
    "    \"LabelAttributeName\": f\"{job_name}-ref\",\n",
    "    \"InputConfig\": {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"ManifestS3Uri\": f\"{scene_upload_s3_prefix}/single.manifest\" \n",
    "            }\n",
    "        },\n",
    "        \"DataAttributes\": {\n",
    "            \"ContentClassifiers\": []\n",
    "        }\n",
    "    },\n",
    "    \"OutputConfig\": {\n",
    "        \"S3OutputPath\": f\"s3://{bucket}/smgt_output\",\n",
    "        \"KmsKeyId\": \"\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"LabelCategoryConfigS3Uri\": label_category_config_s3uri,\n",
    "    \"HumanTaskConfig\": {\n",
    "        \"WorkteamArn\": workteam_arn,\n",
    "        \"UiConfig\": {\n",
    "            \"HumanTaskUiArn\": human_task_ui_arn,\n",
    "        },\n",
    "        \"PreHumanTaskLambdaArn\": prehuman_arn,\n",
    "        \"TaskKeywords\": [\n",
    "            \"Point cloud\",\n",
    "            \"tracking\"\n",
    "        ],\n",
    "        \"TaskTitle\": \"Point cloud object tracking: demo job 10\",\n",
    "        \"TaskDescription\": \"Annotate objects to track their movement across 3D point cloud frames\",\n",
    "        \"NumberOfHumanWorkersPerDataObject\": 1,\n",
    "        \"TaskTimeLimitInSeconds\": 259200,\n",
    "        \"TaskAvailabilityLifetimeInSeconds\": 864000,\n",
    "        \"MaxConcurrentTaskCount\": 1000,\n",
    "        \"AnnotationConsolidationConfig\": {\n",
    "            \"AnnotationConsolidationLambdaArn\": acs_arn\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "res = sagemaker_cl.create_labeling_job(\n",
    "    **request\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134c495",
   "metadata": {},
   "source": [
    "### Enter worker portal\n",
    "\n",
    "Run the following cell to see the worker portal URL of your private workforce. Use this URL to login to the worker portal to see your labeling tasks. See the 3D point cloud worker instructions to learn more about the feature you'll see in the worker UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "workforce = sagemaker_cl.describe_workforce(WorkforceName=\"default\")\n",
    "worker_portal_url = workforce[\"Workforce\"][\"SubDomain\"]\n",
    "print(f\"Sign-in by going here: https://{worker_portal_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec14f0f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we downloaded the A2D2 dataset, reviewed its contents, visualized the 2D and 3D data, and launched a SageMaker Ground Truth labeling job to label our data.\n",
    "\n",
    "\n",
    "Now that we understand our data and its labels, we'll move on to the `sagemaker_training.ipynb` notebook to train a 3D object detection model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
